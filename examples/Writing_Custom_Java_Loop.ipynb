{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Custom Java Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is favorable to use NeuraLogic on a lower level of abstraction and handle the learning loop manually rather than delegating this responsibility to one of the pre-built evaluators.\n",
    "\n",
    "NeuraLogic, especially for the Java Backend, provides multiple ways to handle the learning loop, with different optimizations and levels of control over the loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple problem\n",
    "\n",
    "For the purposes of this tutorial, we will use the following simple problem that describes learning XOR. \n",
    "In contrast to writing loops for different backends, for the Java backend it is important what is the optimizer, learning rate and loss function set in the settings, as related operations are not managed by users in the loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from neuralogic.core.settings import Settings, Optimizer\n",
    "from neuralogic.utils.data import XOR_Vectorized\n",
    "\n",
    "settings = Settings(optimizer=Optimizer.SGD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Getting the NeuraLogic Layer\n",
    "The first step to write custom loop is to get the NeuraLogic that serves for doing the forward propagation of our problem.\n",
    "\n",
    "This can be done by either importing directly the `NeuraLogicLayer` from its module or by using `get_neuralogic_layer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from neuralogic.nn.java import NeuraLogicLayer\n",
    "\n",
    "# or\n",
    "\n",
    "from neuralogic.nn import get_neuralogic_layer\n",
    "from neuralogic.core import Backend\n",
    "\n",
    "NeuraLogicLayer = get_neuralogic_layer(Backend.JAVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to learn our problem, it is required to built it first. Building the problem will do grounding behind the scenes and will process each query and example and yield computation graphs/networks for them as well as learnable parameters (model).\n",
    "\n",
    "It is necessary to specify the backend, because each backend has different needs for the data representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model, dataset = XOR_Vectorized(Backend.JAVA, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to initialize the `NeuraLogicLayer` which will provide us with the interface to do the learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "layer = NeuraLogicLayer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic learning loop\n",
    "The NeuraLogic's learning loop with the Java backend is not a lot different from loops that can be seen in different frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 0.0, Target: 0.0, Loss: 0.0\n",
      "Output: 0.6887123076159376, Target: 1.0, Loss: 0.09690002742979466\n",
      "Output: 0.5538237487509226, Target: 1.0, Loss: 0.19907324717867989\n",
      "Output: 0.8375907129066443, Target: 0.0, Loss: 0.7015582023474607\n"
     ]
    }
   ],
   "source": [
    "epochs = 1  # Specify the number of epochs\n",
    "\n",
    "# Set our model to the training mode\n",
    "# This step is not important for the following use case and can be omitted\n",
    "# Training mode can be also specified using layer(..., train=True) \n",
    "\n",
    "layer.train()\n",
    "\n",
    "for _ in range(epochs):\n",
    "    # Loop over all samples in our dataset\n",
    "    # Samples corresponds to examples and queries \"zipped\" together\n",
    "    for sample in dataset.samples:  \n",
    "        # Do the forward propagation for the sample\n",
    "        loss = layer(sample)\n",
    "        \n",
    "        print(f\"Output: {loss.output()}, Target: {loss.target()}, Loss: {loss.value()}\")\n",
    "\n",
    "        # Do backpropagation\n",
    "        loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizations of the learning loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach described above is correct and offers more control over the learning, but it has some tradeoffs such as overhead as it is required to do multiple calls to Java, which can be expensive.\n",
    "\n",
    "The NeuraLogic framework offers multiple options to deal with scenarios where it is beneficial to gain speed at the cost of loss of the control.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0\n",
      "Loss: 0.09386538738920944\n",
      "Loss: 0.17754162880852017\n",
      "Loss: 0.7099790723897439\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "# In this case, it is important to set the training mode\n",
    "# because otherwise the backpropagation will not be executed\n",
    "layer.train()\n",
    "\n",
    "for _ in range(epochs):\n",
    "    for sample in dataset.samples:\n",
    "        \n",
    "        # Do the forward propagation and also the back propagation using auto_backprop argument\n",
    "        error, _ = layer(sample, auto_backprop=True)\n",
    "        print(f\"Loss: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `auto_backprop` argument in the forward propagation will also do backpropagation without the need of an additional call to the Java backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to optimize the learning loop is to pass all samples to do the forward propagation at once. In this case, the backpropagation will be done without the requirement of the `auto_backprop` argument. The cost of this approach is the loss of some information such as the loss of specific sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.2424097116470788\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "layer.train()  # Again, it is important to set the training mode for the same reasons as above\n",
    "\n",
    "for _ in range(epochs):\n",
    "    # Passing all samples\n",
    "    # Sum of loss/error of all samples will be returned, as well as the number of samples\n",
    "    total_error, num_of_samples = layer(dataset.samples)\n",
    "    \n",
    "    print(f\"Average loss: {total_error / num_of_samples}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be optimized even more by delegating the looping over each epoch to the Java backend. This way, we will only get the information about the total loss and the number of samples in the last epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss of the last epoch 0.23680661462663516\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "layer.train()  # Again, it is important to set the training mode for the same reasons as above\n",
    "\n",
    "# Passing all samples and number of epochs to do\n",
    "total_error, num_of_samples = layer(dataset.samples, epochs=epochs)\n",
    "\n",
    "print(f\"Average loss of the last epoch {total_error / num_of_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to set samples on the layer using `set_training_samples`, which will result in storing those samples on the Java side. Then we can do the forward propagation without any argument and thus without transferring all samples on each epoch to the Java.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.2354814002980244\n",
      "Average loss: 0.23429548825065938\n",
      "Average loss: 0.23320663870237882\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "layer.train()  # Again, it is important to set the training mode for the same reasons as above\n",
    "layer.set_training_samples(dataset.samples)  # Set training samples \n",
    "\n",
    "for _ in range(epochs):\n",
    "    total_error, num_of_samples = layer()  # No arguments\n",
    "    print(f\"Average loss: {total_error / num_of_samples}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Following benchmarks serves as indicators of impacts of different optimizations. It is important to note that not all approaches offer the same level of control and our test problem (vectorized xor) is small. On different (larger) problems, the efficiency might differ more significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from neuralogic import initialize\n",
    "from neuralogic.utils.data import Mutagenesis\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "def prepare_learning():\n",
    "    model, dataset = Mutagenesis(Backend.JAVA, settings)\n",
    "    layer = NeuraLogicLayer(model)\n",
    "    \n",
    "    return dataset, layer\n",
    "\n",
    "\n",
    "def benchmark_learning(fun):\n",
    "    tests = 1\n",
    "    total_time = 0\n",
    "    \n",
    "    for _ in range(tests):\n",
    "        dataset, layer = prepare_learning()\n",
    "        layer.train()\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        fun(dataset, layer)\n",
    "        total_time += time.perf_counter() - start_time\n",
    "    \n",
    "    return (total_time / tests) / epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_learning(dataset, layer):\n",
    "    for _ in range(epochs):\n",
    "        for sample in dataset.samples:\n",
    "            loss = layer(sample)\n",
    "            loss.backward()\n",
    "\n",
    "def classic_auto_backprop_learning(dataset, layer):\n",
    "    for _ in range(epochs):\n",
    "        for sample in dataset.samples:\n",
    "            layer(sample, auto_backprop=True)\n",
    "\n",
    "def sample_batching_learning(dataset, layer):\n",
    "    for _ in range(epochs):\n",
    "        layer(dataset.samples)\n",
    "\n",
    "def sample_batching_learning_delegate_epochs(dataset, layer):\n",
    "    layer(dataset.samples, epochs=epochs)\n",
    "\n",
    "def sample_batching_learning_set_training_samples(dataset, layer):\n",
    "    layer.set_training_samples(dataset.samples)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark Results\n",
    "\n",
    "> Can differ a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classic learning\t\t\t 0.08290630794974277\n"
     ]
    }
   ],
   "source": [
    "print(\"Classic learning\\t\\t\\t\", benchmark_learning(classic_learning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classic learning (auto_backprop=True)\t 0.07494904701015911\n"
     ]
    }
   ],
   "source": [
    "print(\"Classic learning (auto_backprop=True)\\t\", benchmark_learning(classic_auto_backprop_learning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched learning\t\t\t 0.05744525757007068\n"
     ]
    }
   ],
   "source": [
    "print(\"Batched learning\\t\\t\\t\", benchmark_learning(sample_batching_learning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched learning (epochs=...)\t\t 0.05674471384001663\n"
     ]
    }
   ],
   "source": [
    "print(\"Batched learning (epochs=...)\\t\\t\", benchmark_learning(sample_batching_learning_delegate_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched learning (set_training_samples)\t 0.05701841869013151\n"
     ]
    }
   ],
   "source": [
    "print(\"Batched learning (set_training_samples)\\t\", benchmark_learning(sample_batching_learning_set_training_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "### Learning test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_xor_learning():\n",
    "    model, dataset = XOR_Vectorized(Backend.JAVA, settings)\n",
    "    layer = NeuraLogicLayer(model)\n",
    "    \n",
    "    return dataset, layer\n",
    "\n",
    "\n",
    "def evaluate_testing(fun):\n",
    "    dataset, layer = prepare_xor_learning()\n",
    "    layer.train()\n",
    "        \n",
    "    fun(dataset, layer)\n",
    "    \n",
    "    layer.test()\n",
    "    \n",
    "    for sample in dataset.samples:\n",
    "        loss = layer(sample)\n",
    "        print(f\"Output: {loss.output()}, Target: {loss.target()}, Loss: {loss.value()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classic learning\n",
      "Output: 0.0, Target: 0.0, Loss: 0.0\n",
      "Output: 0.7574968842735786, Target: 1.0, Loss: 0.05880776113702211\n",
      "Output: 0.7426854473631991, Target: 1.0, Loss: 0.06621077899867699\n",
      "Output: 0.05052201839625794, Target: 0.0, Loss: 0.0025524743428318253\n"
     ]
    }
   ],
   "source": [
    "print(\"Classic learning\")\n",
    "evaluate_testing(classic_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classic learning (auto_backprop=True)\n",
      "Output: 0.0, Target: 0.0, Loss: 0.0\n",
      "Output: 0.7574968842735786, Target: 1.0, Loss: 0.05880776113702211\n",
      "Output: 0.7426854473631991, Target: 1.0, Loss: 0.06621077899867699\n",
      "Output: 0.05052201839625794, Target: 0.0, Loss: 0.0025524743428318253\n"
     ]
    }
   ],
   "source": [
    "print(\"Classic learning (auto_backprop=True)\")\n",
    "evaluate_testing(classic_auto_backprop_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched learning\n",
      "Output: 0.0, Target: 0.0, Loss: 0.0\n",
      "Output: 0.7574968842735786, Target: 1.0, Loss: 0.05880776113702211\n",
      "Output: 0.7426854473631991, Target: 1.0, Loss: 0.06621077899867699\n",
      "Output: 0.05052201839625794, Target: 0.0, Loss: 0.0025524743428318253\n"
     ]
    }
   ],
   "source": [
    "print(\"Batched learning\")\n",
    "evaluate_testing(sample_batching_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched learning (epochs=...)\n",
      "Output: 0.0, Target: 0.0, Loss: 0.0\n",
      "Output: 0.7574968842735786, Target: 1.0, Loss: 0.05880776113702211\n",
      "Output: 0.7426854473631991, Target: 1.0, Loss: 0.06621077899867699\n",
      "Output: 0.05052201839625794, Target: 0.0, Loss: 0.0025524743428318253\n"
     ]
    }
   ],
   "source": [
    "print(\"Batched learning (epochs=...)\")\n",
    "evaluate_testing(sample_batching_learning_delegate_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched learning (set_training_samples)\n",
      "Output: 0.0, Target: 0.0, Loss: 0.0\n",
      "Output: 0.7574968842735786, Target: 1.0, Loss: 0.05880776113702211\n",
      "Output: 0.7426854473631991, Target: 1.0, Loss: 0.06621077899867699\n",
      "Output: 0.05052201839625794, Target: 0.0, Loss: 0.0025524743428318253\n"
     ]
    }
   ],
   "source": [
    "print(\"Batched learning (set_training_samples)\")\n",
    "evaluate_testing(sample_batching_learning_set_training_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
